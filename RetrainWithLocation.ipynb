{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the training process\n",
    "This notebook is used to examine training process incase you are not familiar with the train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Sequential,Model\n",
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "import os,h5py,sys\n",
    "from tensorflow.python.keras.utils import np_utils\n",
    "from tensorflow.python.keras.datasets import mnist\n",
    "from tensorflow.python.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import random\n",
    "import itertools\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from scipy import misc\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "# from niq.util import memoize\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from text_normalizer.utils.normalize import normalize_text\n",
    "from memoize import memoize\n",
    "# from data_utils import fetch_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_label(input_string):\n",
    "    input_string = input_string\n",
    "    rt = []\n",
    "    for ch in input_string:\n",
    "        if ch in text_label.keys():\n",
    "            lb = text_label[ch]\n",
    "        else:\n",
    "            raise Exception('{} does not exist, textline: {}'.format(ch, input_string))\n",
    "        rt.append(lb)\n",
    "    return rt\n",
    "\n",
    "def fuse_dataset(dataset_1, dataset_2, dataset_1_prob=.8):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            dataset_1: instance of tf.Dataset\n",
    "            dataset_2: instance of tf.Dataset\n",
    "            dataset_1_prob: probability to return dataset1\n",
    "        Returns:\n",
    "            \n",
    "    \"\"\"\n",
    "    tensors_1 = dataset_1.repeat(100).make_one_shot_iterator().get_next()\n",
    "    tensors_2 = dataset_2.repeat(100).make_one_shot_iterator().get_next()\n",
    "    prob_dataset_1 = tf.random_uniform(shape=[]) < dataset_1_prob\n",
    "    features, tar, tar_str, typ = tuple(tf.cond(prob_dataset_1,\n",
    "                                                lambda: (*tensors_1, 'dataset_1'),\n",
    "                                                lambda:(*tensors_2, 'dataset_2'))\n",
    "                                       )\n",
    "    \n",
    "    return features, tar, tar_str, typ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new label_text dictionary\n",
    "label_text = json.load(open('./logdir/box/trained_model/label_text.json', 'r'))\n",
    "label_text = {int(k):v for k,v in label_text.items()}\n",
    "text_label = {v:k for k, v in label_text.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch synthetic paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import fetch_path_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5548/5548 [00:00<00:00, 347230.49it/s]\n",
      "100%|██████████| 35992/35992 [00:00<00:00, 374864.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/dc3/test/ 5548\n",
      "./datasets/dc3/train/ 35992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:00<00:00, 360310.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/systhetic_from_text_train/ 100000\n"
     ]
    }
   ],
   "source": [
    "data_dict_test = fetch_path_raw('./datasets/dc3/test/')\n",
    "data_dict_train = fetch_path_raw('./datasets/dc3/train/')\n",
    "data_dict_synthetic_test = fetch_path_raw('./datasets/systhetic_from_text_train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_kankush = fetch_path_raw('./datasets/kankuset/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_train = data_dict_kankush + data_dict_synthetic_test*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset_location(ann_paths, batch_size = 2):\n",
    "    \"\"\"\n",
    "        inputs: fetch path\n",
    "        returns: tf dataset of, features, label_np, list_location, raw_path\n",
    "    \"\"\"\n",
    "    from data_utils import read_image    \n",
    "    def map_func(ann_path):\n",
    "\n",
    "        with open(ann_path, 'r') as  f:\n",
    "            lines = f.readlines()\n",
    "            location_dict = eval(lines[0])\n",
    "\n",
    "        path = ann_path[:-4]\n",
    "\n",
    "        locations = []\n",
    "        characters = []\n",
    "        start_location = 0\n",
    "        text = ''.join([_[-1] for _ in location_dict])\n",
    "        strip_text = text.strip()\n",
    "        s_strip = text.index(strip_text)\n",
    "        e_strip = s_strip+len(strip_text) \n",
    "\n",
    "        for x1,x2, c in location_dict:\n",
    "            start_location = x1//4\n",
    "            end_location = x2//4\n",
    "            locations.append([start_location, end_location])\n",
    "            norm_char = normalize_text(c)\n",
    "            norm_char_label = convert_text_label(norm_char)\n",
    "            characters.append(norm_char_label)\n",
    "\n",
    "        max_l = max([len(_) for _ in characters])\n",
    "        characters_np = np.ones([len(characters), max_l], dtype=np.float32)*-1\n",
    "        for i, character in enumerate(characters):\n",
    "            characters_np[i,:len(character)] = character\n",
    "        locations = np.array(locations, dtype=np.int32)\n",
    "        return read_image(path), characters_np, locations , path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(ann_paths)\n",
    "    dataset = dataset.map(lambda item1: tf.py_func(\n",
    "              map_func, [item1], [tf.float32, tf.float32, tf.int32, tf.string]), num_parallel_calls=8)\n",
    "\n",
    "    dataset = dataset.padded_batch(batch_size, padded_shapes=([1, None, 512], [None, None], [None,2], []), \n",
    "                                        padding_values=(0., -1., -1, ''))\n",
    "\n",
    "    dataset = dataset.prefetch(100)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import read_image\n",
    "from pyson.vision import resize_by_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_string(string):\n",
    "    for ch in string:\n",
    "        if not ch in text_label.keys():\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_dataset(tupple_path_annotations, batch_size = 16):\n",
    "    \n",
    "    def shuffle(paths, batch_size):\n",
    "        rts = []\n",
    "        for i in range(0,len(paths), batch_size):\n",
    "            rts.append(paths[i:i+batch_size])\n",
    "        np.random.shuffle(rts)\n",
    "        rt = []\n",
    "        for _ in rts:\n",
    "            rt.extend(_)\n",
    "        return rt\n",
    "    \n",
    "    def map_func(tupple_path_annotation):\n",
    "        path, ann = eval(tupple_path_annotation)\n",
    "        label_string = normalize_text(ann['text'])\n",
    "        path = tf.compat.as_text(path,encoding='utf-8')\n",
    "        label_number = convert_text_label(label_string)\n",
    "        label_number_np = np.ones([len(label_number)]) * -1\n",
    "        label_number_np[:] = label_number\n",
    "        \n",
    "        img = read_image(path)\n",
    "        img = resize_by_factor(img, 48/img.shape[0])\n",
    "        img = np.expand_dims(img, -1)\n",
    "        img = img.astype(np.float32)\n",
    "        return img, label_number_np.astype(np.float32)\n",
    "    \n",
    "    tupple_path_annotations = [_ for _ in tupple_path_annotations if is_valid_string(_)]\n",
    "    \n",
    "    tupple_path_annotations = shuffle(tupple_path_annotations, batch_size)\n",
    "    \n",
    "    tupple_path_annotations = [str(_) for _ in tupple_path_annotations]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(tupple_path_annotations)\n",
    "    dataset = dataset.map(lambda item1: tf.py_func(\n",
    "              map_func, [item1], [tf.float32, tf.float32]), num_parallel_calls=8)\n",
    "\n",
    "    dataset = dataset.padded_batch(batch_size, padded_shapes=([48, None, 1], [None]), \n",
    "                                        padding_values=(0., -1.))\n",
    "    \n",
    "    dataset = dataset.prefetch(100)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = create_tf_dataset(data_dict_train, batch_size=batch_size)\n",
    "dataset_test = create_tf_dataset(data_dict_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label_number_np = dataset_test.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.keras.backend.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyson.vision import plot_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_img,_label_number_np = sess.run([img, label_number_np])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_labels = []\n",
    "for line in _label_number_np.astype(int):\n",
    "    text = ''\n",
    "    for ch in line:\n",
    "        text += label_text[ch] if ch in label_text.keys() else ''\n",
    "    _labels.append(text,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(_img[...,0], mxn=(16, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ctc_bestpath import ctcBestPath\n",
    "\n",
    "def dump_model_config(output_path, model):\n",
    "    config = model.get_config()\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(config, f)\n",
    "    \n",
    "\n",
    "def caculate_error_rate(features, tar_string):\n",
    "    o = predict_with_session(features)\n",
    "    t = [x.decode('utf-8') for x in tar_string]\n",
    "    outs = np.array([edit_distance(a,b) for a, b in zip(t, o)])\n",
    "    outs = outs.clip(0,1)\n",
    "    return 1-outs.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create inputs tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(kush_model, decoder):\n",
    "    input = kush_model.input\n",
    "    x = input\n",
    "    for layer in kush_model.layers[:-1]:\n",
    "        x = layer(x)\n",
    "    return tf.keras.models.Model(input, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K.clear_session()\n",
    "# tf.reset_default_graph()\n",
    "# sess = K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 48, None, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 48, None, 64)      640       \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 24, None, 64)      0         \n",
      "_________________________________________________________________\n",
      "drop1 (Dropout)              (None, 24, None, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 24, None, 128)     73856     \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 12, None, 128)     0         \n",
      "_________________________________________________________________\n",
      "drop2 (Dropout)              (None, 12, None, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 12, None, 256)     295168    \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 12, None, 256)     590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 12, None, 256)     0         \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling2D)         (None, 6, None, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv5 (Conv2D)               (None, 6, None, 512)      1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 6, None, 512)      2048      \n",
      "_________________________________________________________________\n",
      "conv6 (Conv2D)               (None, 6, None, 512)      2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 6, None, 512)      2048      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 6, None, 512)      0         \n",
      "_________________________________________________________________\n",
      "pool4 (MaxPooling2D)         (None, 3, None, 512)      0         \n",
      "_________________________________________________________________\n",
      "conv7 (Conv2D)               (None, 1, None, 512)      2359808   \n",
      "_________________________________________________________________\n",
      "permute (Permute)            (None, None, 1, 512)      0         \n",
      "_________________________________________________________________\n",
      "timedistrib (TimeDistributed (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, None, 512)         2048      \n",
      "_________________________________________________________________\n",
      "blstm1 (Bidirectional)       (None, None, 512)         1182720   \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, None, 512)         2048      \n",
      "_________________________________________________________________\n",
      "blstm1_out (Dense)           (None, None, 256)         131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, None, 256)         1024      \n",
      "_________________________________________________________________\n",
      "blstm2 (Bidirectional)       (None, None, 512)         789504    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, None, 4166)        2137158   \n",
      "=================================================================\n",
      "Total params: 11,109,446\n",
      "Trainable params: 9,001,286\n",
      "Non-trainable params: 2,108,160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "weights_path = './logdir/box/trained_model/basemodel_retrain_50000.h5'\n",
    "config_path = './logdir/box/trained_model/basemodel.json'\n",
    "\n",
    "\n",
    "def basemodel_with_batch_norm(basemodel_old):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    timedistrib (TimeDistributed (None, None, 512)         0         \n",
    "    _________________________________________________________________\n",
    "    blstm1 (Bidirectional)       (None, None, 512)         1182720   \n",
    "    _________________________________________________________________\n",
    "    blstm1_out (Dense)           (None, None, 256)         131328    \n",
    "    _________________________________________________________________\n",
    "    blstm2 (Bidirectional)       (None, None, 512)         789504    \n",
    "    _________________________________________________________________\n",
    "    dense (Dense)                (None, None, 4166)        2137158   \n",
    "    ================================================================\n",
    "'''\n",
    "    train_vars_list = []\n",
    "    input = basemodel_old.input\n",
    "    x = input\n",
    "    for layer in basemodel_old.layers[1:]:\n",
    "        if layer.name == 'timedistrib':\n",
    "            break\n",
    "        x = layer(x)\n",
    "        \n",
    "    x = basemodel_old.get_layer('timedistrib').output\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = basemodel_old.get_layer('blstm1')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = basemodel_old.get_layer('blstm1_out')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = basemodel_old.get_layer('blstm2')(x)\n",
    "    x = basemodel_old.get_layer('dense')(x)\n",
    "    basemodel = tf.keras.models.Model(input, x)\n",
    "    is_train_layer = False\n",
    "    for layer in basemodel.layers:\n",
    "        if layer.name == 'timedistrib':\n",
    "#             import ipdb; ipdb.set_trace()\n",
    "            is_train_layer = True\n",
    "        if is_train_layer:\n",
    "            train_vars_list.extend(layer.weights)\n",
    "            \n",
    "    return basemodel, train_vars_list\n",
    "\n",
    "config = json.load(open(config_path, 'r'))\n",
    "basemodel_4166 = tf.keras.models.Model.from_config(config)\n",
    "basemodel_4166.load_weights(weights_path)\n",
    "\n",
    "basemodel, train_vars_list = basemodel_with_batch_norm(basemodel_4166)\n",
    "\n",
    "basemodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'batch_normalization_29/gamma:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'batch_normalization_29/beta:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'batch_normalization_29/moving_mean:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'batch_normalization_29/moving_variance:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'blstm1_24/forward_cu_dnngru_6/kernel:0' shape=(512, 768) dtype=float32>,\n",
       " <tf.Variable 'blstm1_24/forward_cu_dnngru_6/recurrent_kernel:0' shape=(256, 768) dtype=float32>,\n",
       " <tf.Variable 'blstm1_24/forward_cu_dnngru_6/bias:0' shape=(1536,) dtype=float32>,\n",
       " <tf.Variable 'blstm1_24/backward_cu_dnngru_6/kernel:0' shape=(512, 768) dtype=float32>,\n",
       " <tf.Variable 'blstm1_24/backward_cu_dnngru_6/recurrent_kernel:0' shape=(256, 768) dtype=float32>,\n",
       " <tf.Variable 'blstm1_24/backward_cu_dnngru_6/bias:0' shape=(1536,) dtype=float32>,\n",
       " <tf.Variable 'batch_normalization_30/gamma:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'batch_normalization_30/beta:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'batch_normalization_30/moving_mean:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'batch_normalization_30/moving_variance:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'blstm1_out_23/kernel:0' shape=(512, 256) dtype=float32>,\n",
       " <tf.Variable 'blstm1_out_23/bias:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'batch_normalization_31/gamma:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'batch_normalization_31/beta:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'batch_normalization_31/moving_mean:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'batch_normalization_31/moving_variance:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'blstm2_24/forward_cu_dnngru_7/kernel:0' shape=(256, 768) dtype=float32>,\n",
       " <tf.Variable 'blstm2_24/forward_cu_dnngru_7/recurrent_kernel:0' shape=(256, 768) dtype=float32>,\n",
       " <tf.Variable 'blstm2_24/forward_cu_dnngru_7/bias:0' shape=(1536,) dtype=float32>,\n",
       " <tf.Variable 'blstm2_24/backward_cu_dnngru_7/kernel:0' shape=(256, 768) dtype=float32>,\n",
       " <tf.Variable 'blstm2_24/backward_cu_dnngru_7/recurrent_kernel:0' shape=(256, 768) dtype=float32>,\n",
       " <tf.Variable 'blstm2_24/backward_cu_dnngru_7/bias:0' shape=(1536,) dtype=float32>,\n",
       " <tf.Variable 'dense_23/kernel:0' shape=(512, 4166) dtype=float32>,\n",
       " <tf.Variable 'dense_23/bias:0' shape=(4166,) dtype=float32>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vars_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels = dataset_train.repeat(100).make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_labels = dataset_test.repeat(100).make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyson.losses import suppervise_ctc_loss, ctc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = basemodel_4166(train_images)\n",
    "\n",
    "normal_ctc_loss = ctc_loss(train_labels, train_preds)\n",
    "loss = normal_ctc_loss\n",
    "\n",
    "\n",
    "global_step = tf.Variable(0, dtype=tf.int64, name='global_step')\n",
    "train_step = tf.train.AdamOptimizer().minimize(loss, var_list=basemodel_4166.layers[-1].weights, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel_4166.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {'supervise_loss':[], 'normal_ctc_loss': [], 'train_acc':[], 'test_acc':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pretty_summary():\n",
    "    s = ''\n",
    "    for k, v in losses.items():\n",
    "        s += '{}:{:0.4f}\\t'.format(k, np.mean(v[-100:]))\n",
    "    return s\n",
    "\n",
    "def update_summary(supervise_loss, normal_ctc_loss, train_acc, test_acc):\n",
    "    losses['supervise_loss'].append(supervise_loss)\n",
    "    losses['normal_ctc_loss'].append(normal_ctc_loss)\n",
    "    losses['train_acc'].append(train_acc)\n",
    "    losses['test_acc'].append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = basemodel_4166(test_images)\n",
    "def caculate_error_rate(mode):\n",
    "    global test_preds\n",
    "    global train_preds\n",
    "    def get_texts(tar):\n",
    "        texts = []\n",
    "        for line in tar:\n",
    "            text = ''\n",
    "            line = np.reshape(line, [-1]).astype(int)\n",
    "            for ichar in line:\n",
    "                if ichar != -1:\n",
    "                    text += label_text[ichar]\n",
    "            texts.append(text)\n",
    "        return texts\n",
    "    \n",
    "    if mode == 'test':\n",
    "        o, t = sess.run([test_preds, test_labels])\n",
    "    elif mode == 'train':\n",
    "        o, t = sess.run([train_preds, train_labels])\n",
    "    else:\n",
    "        raise Exception('not yet implemented')\n",
    "    t = get_texts(t)#[_.decode('utf-8') for _ in t]\n",
    "    o = [ctcBestPath(_, label_text) for _ in o]\n",
    "    outs = np.array([edit_distance(a,b) for a, b in zip(t, o)])\n",
    "    outs = outs.clip(0,1)\n",
    "    return 1-outs.mean(), t, o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDIR = 'logdir/28_feb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.summary.scalar('supervise_loss', supervise_loss)\n",
    "tf.summary.scalar('normal_ctc_loss', normal_ctc_loss)\n",
    "merged = tf.summary.merge_all()\n",
    "tensor_dir = LOGDIR + '/tb'\n",
    "os.system('rm -r {}/*'.format(tensor_dir))\n",
    "train_writer = tf.summary.FileWriter(tensor_dir,sess.graph)\n",
    "print('RUN: tensorboard --logdir {} --port 5001'.format(os.path.abspath(LOGDIR)))\n",
    "summary = tf.Summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_set(data_dict):\n",
    "    texts = [_[1]['text'] for _ in data_dict]\n",
    "    chars = ''.join(texts)\n",
    "    chars_norm = normalize_text(chars)\n",
    "    return set(chars_norm)\n",
    "train_chars = get_char_set(data_dict_train)\n",
    "test_chars = get_char_set(data_dict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chars_kan = get_char_set(data_dict_kankush)\n",
    "print(len(train_chars), len(test_chars))\n",
    "unavail = test_chars - train_chars - train_chars_kan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text in  [_[1]['text'] for _ in data_dict_test]:\n",
    "#     for ch in unavail:\n",
    "#         if ch in text:\n",
    "#             print(text, '\\t', ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_freq = 5\n",
    "start = time()\n",
    "init_step = sess.run(global_step)\n",
    "for step in range(init_step, int(32e5)):\n",
    "    train_dict = {'train_step':train_step}\n",
    "    if step % display_freq == 0:\n",
    "        train_dict['norm_loss'] = normal_ctc_loss\n",
    "    \n",
    "    results = sess.run(train_dict)\n",
    "    if step % display_freq == 0:\n",
    "        speed = (step-init_step) * batch_size / (time()-start)\n",
    "        _summary = sess.run(merged)\n",
    "        train_writer.add_summary(_summary, step)\n",
    "        train_accuracy = caculate_error_rate('train')[0]\n",
    "        test_accuracy = caculate_error_rate('train')[0]\n",
    "        summary.value.add(tag='Train Accuracy', simple_value=train_accuracy)\n",
    "        summary.value.add(tag='Test Accuracy', simple_value=test_accuracy)\n",
    "\n",
    "#         _supervise_loss, _normal_loss = K.get_session().run([supervise_loss, normal_ctc_loss])        \n",
    "        update_summary(0, results['norm_loss'], train_accuracy, test_accuracy)\n",
    "        pretty_summary()\n",
    "        \n",
    "        print('\\r Step: {}\\t\\t Speed: {:0.2f}\\t {}'.format(step, speed, pretty_summary()), end='')\n",
    "        train_writer.add_summary(summary, step)\n",
    "\n",
    "#     if step % 5000 == 0 and step > 100:\n",
    "#         save_path = os.path.join(LOGDIR, 'basemodel_retrain_{}.h5'.format(step))\n",
    "#         print('save at: ', save_path)\n",
    "#         basemodel.save_weights(save_path)\n",
    "#         saver.save(sess, LOGDIR, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(LOGDIR, 'basemodel_retrain_{}.h5'.format(step))\n",
    "print('save at: ', save_path)\n",
    "basemodel.save_weights(save_path)\n",
    "# saver.save(sess, LOGDIR, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from glob import glob\n",
    "# paths = glob('./datasets/kankuset/train/raw/*.kush.npy')\n",
    "# print(len(paths))\n",
    "# from tqdm import tqdm\n",
    "# import os\n",
    "# !mkdir ./datasets/kankuset/train/kush\n",
    "# for path in tqdm(paths):\n",
    "#     new_path = paths[0].replace('.kush', '').replace('/raw/', '/kush/')#paths[0].strip('.kush').replace('/raw/','/kush/')\n",
    "#     os.rename(path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
